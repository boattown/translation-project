{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "historical-script",
   "metadata": {},
   "source": [
    "# Machine translation Nahuatl - Spanish\n",
    "\n",
    "Data from Axolotl parallel corpus: https://axolotl-corpus.mx/\n",
    "\n",
    "COURSE PROJECT LT2326\n",
    "\n",
    "Oct 2021\n",
    "\n",
    "Klara Båstedt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-desperate",
   "metadata": {},
   "source": [
    "### Part 1 - Data preparation\n",
    "\n",
    "The data is a parallel corpus of texts in Spanish and Nahuatl consisting of almost 18000 parallel sentences.\n",
    "\n",
    "Spelling normalization: https://pypi.org/project/elotl/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-league",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import elotl.corpus\n",
    "import elotl.nahuatl.orthography\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-imagination",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install elotl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "double-upset",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {'epochs':3,\n",
    "                   'batch_size':16,\n",
    "                   'embedding_size':128,\n",
    "                   'hidden_size':1024,\n",
    "                   'learning_rate':0.001,\n",
    "                   'num_layers':2,\n",
    "                   'dropout':0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "commercial-album",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv(\"Axolotl.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-providence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_nahuatl(x):\n",
    "    n = elotl.nahuatl.orthography.Normalizer(\"sep\")\n",
    "    return n.normalize(x)\n",
    "\n",
    "def remove_punct(x):\n",
    "    string.punctuation = string.punctuation + '¿'\n",
    "    exclude = set(string.punctuation)\n",
    "    x.translate(str.maketrans('', '', string.punctuation))\n",
    "    stripped_string = ''.join(ch for ch in x if ch not in exclude)\n",
    "    return stripped_string.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "burning-rwanda",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus['Nah'] = corpus['Nah'].apply(normalize_nahuatl)\n",
    "corpus['Esp'] = corpus['Esp'].apply(remove_punct)\n",
    "corpus['Nah'] = corpus['Nah'].apply(remove_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-crack",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = shuffle(corpus)\n",
    "corpus.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpreted-manner",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = corpus[:16995]\n",
    "val_df = corpus[16995:17445]\n",
    "test_df = corpus[17445:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-witness",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"train.csv\", index=False)\n",
    "val_df.to_csv(\"val.csv\", index=False)\n",
    "test_df.to_csv(\"test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-singapore",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-testament",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "available-marijuana",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-death",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    whitespacer = lambda x: x.split(' ')\n",
    "\n",
    "    SPANISH = Field(\n",
    "        tokenize=whitespacer,\n",
    "        lower=True,                   \n",
    "        batch_first=False,\n",
    "        init_token='<start>',\n",
    "        eos_token='<end>')\n",
    "    \n",
    "    NAHUATL = Field(\n",
    "        tokenize=whitespacer,\n",
    "        lower=True,                   \n",
    "        batch_first=False,\n",
    "        init_token='<start>',\n",
    "        eos_token='<end>')\n",
    "    \n",
    "    train, val, test = TabularDataset.splits(\n",
    "                        path = './',\n",
    "                        train = 'train.csv',\n",
    "                        validation = 'val.csv',\n",
    "                        test = 'test.csv',\n",
    "                        format = 'csv',\n",
    "                        fields = [('spanish', SPANISH), ('nahuatl', NAHUATL)],\n",
    "                        skip_header = True)\n",
    "    \n",
    "    SPANISH.build_vocab(train, val)\n",
    "    NAHUATL.build_vocab(train, val)\n",
    "\n",
    "    \n",
    "    train_iter = BucketIterator(\n",
    "        train,                                                  \n",
    "        batch_size=hyperparameters['batch_size'],\n",
    "        sort_within_batch=True,\n",
    "        sort_key=lambda x: (len(x.nahuatl)),\n",
    "        shuffle=True,                                                  \n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    val_iter = BucketIterator(\n",
    "        val,                                                  \n",
    "        batch_size=hyperparameters['batch_size'],\n",
    "        sort_within_batch=True,\n",
    "        sort_key=lambda x: (len(x.nahuatl)),\n",
    "        shuffle=True,                                                  \n",
    "        device=device\n",
    "    )\n",
    "                \n",
    "    test_iter = BucketIterator(\n",
    "        test,                                                  \n",
    "        batch_size=hyperparameters['batch_size'],\n",
    "        sort_within_batch=True,\n",
    "        sort_key=lambda x: (len(x.nahuatl)),\n",
    "        shuffle=True,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    return train_iter, val_iter, test_iter, NAHUATL, SPANISH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-prayer",
   "metadata": {},
   "source": [
    "SPANISH.build_vocab(train, val, max_size=10000, min_freq=2)\n",
    "NAHUATL.build_vocab(train, val, max_size=10000, min_freq=2)\n",
    "\n",
    "The size of len(SPANISH.vocab) and len(NAHUATL.vocab) is 13982 and 16399, with min_freq=2.\n",
    "Nahuatl is agglutinative while Spanish is not. This explains why the sizes of the vocabularies change to 27313 and 49302 when min_freq=1. I don't think min_freq=2 works well when one language is agglutinative. We will need our network to have access to as many nahuan \"words\" as possible. For the same reason it's not smart to set the vocabularies to equal size.\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-marker",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter, NAHUATL, SPANISH = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-attachment",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(NAHUATL.vocab.stoi[\"auakatl\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amber-juice",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(NAHUATL.vocab.itos[17656])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-morrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SPANISH.vocab.stoi[\"aguacate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hazardous-italy",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SPANISH.vocab.itos[5543])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "healthy-velvet",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(SPANISH.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-details",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(NAHUATL.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inappropriate-inventory",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "universal-marker",
   "metadata": {},
   "source": [
    "### Part 2 - Building and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-floating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testa att inkludera validation i train-loopen och att sluta träna när loss inte längre minskar\n",
    "# Testa att ta med en exempelmening och se hur den blir bättre översatt för varje epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-satellite",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-satisfaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, hidden_size, num_layers, drop):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.lstm = nn.LSTM(emb_size, hidden_size, num_layers, dropout=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # shape of x: (length, batchsize)\n",
    "        x1 = self.embedding(x)\n",
    "        # shape of x: (length, batchsize, embsize)\n",
    "        x2 = self.dropout(x1)\n",
    "        output, (hidden, cell) = self.lstm(x2)\n",
    "\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-constitutional",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, hidden_size, num_layers, drop):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.lstm = nn.LSTM(emb_size, hidden_size, num_layers, dropout=drop)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden, cell):\n",
    "        #shape of x: (N), but we want (1, N) or \n",
    "        x = x.unsqueeze(0)\n",
    "        x2 = self.embedding(x)\n",
    "        x3 = self.dropout(x2)\n",
    "        output, (hidden, cell) = self.lstm(x3, (hidden, cell))\n",
    "        x4 = self.fc(output)\n",
    "        x4 = x4.squeeze(0)\n",
    "        return x4, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-theater",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
    "        batch_size = source.shape[1]\n",
    "        target_length = target.shape[0]\n",
    "        target_vocab_size = self.decoder.vocab_size\n",
    "        sentence = torch.zeros(target_length, batch_size, target_vocab_size).to(device)\n",
    "        hidden, cell = self.encoder(source)\n",
    "        x = target[0]\n",
    "        \n",
    "        for t in range(1, target_length):\n",
    "            \n",
    "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
    "            sentence[t] = output\n",
    "            teacher_force = random.random() < teacher_force_ratio\n",
    "            predicted_word = output.argmax(1) \n",
    "            x = target[t] if teacher_force else predicted_word\n",
    "            \n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-throw",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brilliant-edmonton",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(len(NAHUATL.vocab), \n",
    "                  hyperparameters[\"embedding_size\"], \n",
    "                  hyperparameters[\"hidden_size\"], \n",
    "                  hyperparameters[\"num_layers\"],\n",
    "                  hyperparameters[\"dropout\"]).to(device)\n",
    "\n",
    "decoder = Decoder(len(SPANISH.vocab), \n",
    "                  hyperparameters[\"embedding_size\"], \n",
    "                  hyperparameters[\"hidden_size\"], \n",
    "                  hyperparameters[\"num_layers\"], \n",
    "                  hyperparameters[\"dropout\"]).to(device)\n",
    "                            \n",
    "seq2seq = Seq2Seq(encoder, decoder).to(device)\n",
    "\n",
    "# Lägg till nåt här så loss inte räknas på padding\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(\n",
    "    seq2seq.parameters(),\n",
    "    lr=hyperparameters['learning_rate']\n",
    ")\n",
    "\n",
    "# start training loop\n",
    "total_loss = 0\n",
    "for epoch in range(hyperparameters['epochs']):\n",
    "    for i, batch in enumerate(train_iter):\n",
    "        source = batch.nahuatl.to(device)\n",
    "        target = batch.spanish.to(device)\n",
    "        output = seq2seq(source, target)\n",
    "        # shape (trglength, batchsize, outputdim)\n",
    "        loss = loss_fn(output[1:].reshape(-1, output.shape[2]), target[1:].reshape(-1))\n",
    "        total_loss += loss.item()\n",
    "        print(f\"Loss in epoch {epoch+1} is:\" total_loss/(i+1), end='\\r')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sixth-ethnic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-praise",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-flight",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lstm_model.state_dict(), \"|\".join([f\"{k}_{v}\" for k, v in model_hyperparameters.items()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-operator",
   "metadata": {},
   "source": [
    "### Part 3 - Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-bibliography",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bleu score\n",
    "# back translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-disorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "total_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(test_iter):\n",
    "        source = batch.nahuatl.to(device)\n",
    "        target = batch.spanish.to(device)\n",
    "        output = seq2seq(source, target)\n",
    "        # shape (trglength, batchsize, outputdim)\n",
    "        loss = loss_fn(output[1:].reshape(-1, output.shape[2]), target[1:].reshape(-1))\n",
    "        total_loss += loss.item()\n",
    "        print(f\"Loss in epoch {epoch+1} is:\" total_loss/(i+1), end='\\r')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imported-suggestion",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-princess",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-selection",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-stake",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "downtown-african",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "https://axolotl-corpus.mx/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
