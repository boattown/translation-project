{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "historical-script",
   "metadata": {},
   "source": [
    "# Machine translation Nahuatl - Spanish\n",
    "\n",
    "COURSE PROJECT LT2326\n",
    "November 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-desperate",
   "metadata": {},
   "source": [
    "### Part 1 - Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f98c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install elotl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "entire-league",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext.legacy.data import Field, TabularDataset, BucketIterator\n",
    "from torchtext.data.metrics import bleu_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import elotl.corpus\n",
    "import elotl.nahuatl.orthography\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "double-upset",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\"epochs\":20,\n",
    "                   \"batch_size\":64,\n",
    "                   \"embedding_size\":256,\n",
    "                   \"hidden_size\":1024,\n",
    "                   \"learning_rate\":0.001,\n",
    "                   \"num_layers\":2,\n",
    "                   \"dropout\":0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcf8ba36",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:3\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "commercial-album",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv(\"Axolotl.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-providence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions remove punctuation and normalize Nahuan spelling.\n",
    "\n",
    "def normalize_nahuatl(x):\n",
    "    n = elotl.nahuatl.orthography.Normalizer(\"inali\")\n",
    "    return n.normalize(x)\n",
    "\n",
    "def remove_punct(x):\n",
    "    string.punctuation = string.punctuation + '¿'\n",
    "    exclude = set(string.punctuation)\n",
    "    x.translate(str.maketrans('', '', string.punctuation))\n",
    "    stripped_string = ''.join(ch for ch in x if ch not in exclude)\n",
    "    return stripped_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "burning-rwanda",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus['Nah'] = corpus['Nah'].apply(normalize_nahuatl)\n",
    "corpus['Esp'] = corpus['Esp'].apply(remove_punct)\n",
    "corpus['Nah'] = corpus['Nah'].apply(remove_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba738f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3aa62e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = corpus.Esp.str.len().sort_values().index\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5003e0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.reindex(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff6a684",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "for idx, row in corpus.reindex(s).iterrows():\n",
    "    if len(row[\"Esp\"].split()) > 75:\n",
    "        n += 1\n",
    "        print(row[\"Esp\"])\n",
    "        print()\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbcadc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code removes the 824 sentence pairs with more than 75 words in their Spanish version.\n",
    "\n",
    "for idx, row in corpus.reindex(s).iterrows():\n",
    "    if len(row[\"Esp\"].split()) > 75:\n",
    "        corpus = corpus.drop(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2d8d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "for idx, row in corpus.iterrows():\n",
    "    if len(row[\"Esp\"].split()) > 75:\n",
    "        n += 1\n",
    "        print(row[\"Esp\"])\n",
    "        print()\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-crack",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = shuffle(corpus)\n",
    "corpus.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a24d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpreted-manner",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = corpus[:16000]\n",
    "val_df = corpus[16000:16500]\n",
    "test_df = corpus[16500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a6c822",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_df))\n",
    "print(len(val_df))\n",
    "print(len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442dc18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-witness",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"train.csv\", index=False)\n",
    "val_df.to_csv(\"val.csv\", index=False)\n",
    "test_df.to_csv(\"test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-testament",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.asarray([len(sent.split()) for sent in corpus[\"Esp\"]])\n",
    "\n",
    "fig,ax = plt.subplots(1,1)\n",
    "ax.hist(x, bins = [0,5,10,15,20,25,30,35,40,45,50,60,70,80,90,100,110,120,130,140,150,175,200,225,250,275,300])\n",
    "ax.set_title(\"Lengths of Spanish sentences\")\n",
    "#ax.set_xticks([0,10,20,30,40,50,60,70,80,90,100,150,200,250,300])\n",
    "ax.set_xlabel(\"Number of words in sentence\")\n",
    "ax.set_ylabel(\"Number of sentences\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Longest sentence in corpus: {x[x.argmax()]} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "rental-death",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function opens the csv-files with the datasets and creates train, validation and test iterators.\n",
    "# The Spanish and Nahuan sentences are tokenized and their vocabularies created where each unique token is assigned an index.\n",
    "\n",
    "def get_data():\n",
    "    whitespacer = lambda x: x.split(' ')\n",
    "\n",
    "    SPANISH = Field(\n",
    "        tokenize=whitespacer,\n",
    "        lower=True,                   \n",
    "        batch_first=False,\n",
    "        init_token=\"<start>\",\n",
    "        eos_token=\"<end>\")\n",
    "    \n",
    "    NAHUATL = Field(\n",
    "        tokenize=whitespacer,\n",
    "        lower=True,                   \n",
    "        batch_first=False,\n",
    "        init_token=\"<start>\",\n",
    "        eos_token=\"<end>\")\n",
    "    \n",
    "    train, val, test = TabularDataset.splits(\n",
    "                        path = \"./\",\n",
    "                        train = \"train.csv\",\n",
    "                        validation = \"val.csv\",\n",
    "                        test = \"test.csv\",\n",
    "                        format = \"csv\",\n",
    "                        fields = [(\"spanish\", SPANISH), (\"nahuatl\", NAHUATL)],\n",
    "                        skip_header = True)\n",
    "    \n",
    "    SPANISH.build_vocab(train, val, min_freq=2)\n",
    "    NAHUATL.build_vocab(train, val, min_freq=2)\n",
    "\n",
    "    \n",
    "    train_iter = BucketIterator(\n",
    "        train,                                                  \n",
    "        batch_size=hyperparameters[\"batch_size\"],\n",
    "        sort_within_batch=True,\n",
    "        sort_key=lambda x: (len(x.nahuatl)),\n",
    "        shuffle=True,                                                  \n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    val_iter = BucketIterator(\n",
    "        val,                                                  \n",
    "        batch_size=hyperparameters[\"batch_size\"],\n",
    "        sort_within_batch=True,\n",
    "        sort_key=lambda x: (len(x.nahuatl)),\n",
    "        shuffle=True,                                                  \n",
    "        device=device\n",
    "    )\n",
    "                \n",
    "    test_iter = BucketIterator(\n",
    "        test,                                                  \n",
    "        batch_size=hyperparameters[\"batch_size\"],\n",
    "        sort_within_batch=True,\n",
    "        sort_key=lambda x: (len(x.nahuatl)),\n",
    "        shuffle=True,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    return train_iter, val_iter, test_iter, NAHUATL, SPANISH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "jewish-marker",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter, NAHUATL, SPANISH = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "constitutional-attachment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "print(NAHUATL.vocab.stoi[\"nikan\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "amber-juice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nikan\n"
     ]
    }
   ],
   "source": [
    "print(NAHUATL.vocab.itos[19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "composite-morrison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3543\n"
     ]
    }
   ],
   "source": [
    "print(SPANISH.vocab.stoi[\"aguacate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "hazardous-italy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aguacate\n"
     ]
    }
   ],
   "source": [
    "print(SPANISH.vocab.itos[3543])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "healthy-velvet",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10451"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(SPANISH.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "hundred-details",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12845"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(NAHUATL.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b289812f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n"
     ]
    }
   ],
   "source": [
    "print(len(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a03c02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(len(val_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e189b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(len(test_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-marker",
   "metadata": {},
   "source": [
    "### Part 2 - Building and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-satisfaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, hidden_size, num_layers, drop):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.lstm = nn.LSTM(emb_size, hidden_size, num_layers, dropout=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.embedding(x)\n",
    "        x2 = self.dropout(x1)\n",
    "        output, (hidden, cell) = self.lstm(x2)\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-constitutional",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, hidden_size, num_layers, drop):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.lstm = nn.LSTM(emb_size, hidden_size, num_layers, dropout=drop)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden, cell):\n",
    "        x = x.unsqueeze(0)\n",
    "        x2 = self.embedding(x)\n",
    "        x3 = self.dropout(x2)\n",
    "        output, (hidden, cell) = self.lstm(x3, (hidden, cell))\n",
    "        x4 = self.fc(output)\n",
    "        x4 = x4.squeeze(0)\n",
    "        return x4, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-theater",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
    "        batch_size = source.shape[1]\n",
    "        target_length = target.shape[0]\n",
    "        target_vocab_size = self.decoder.vocab_size\n",
    "        sentence = torch.zeros(target_length, batch_size, target_vocab_size).to(device)\n",
    "        hidden, cell = self.encoder(source)\n",
    "        x = target[0]\n",
    "        \n",
    "        for t in range(1, target_length):\n",
    "            \n",
    "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
    "            sentence[t] = output\n",
    "            teacher_force = random.random() < teacher_force_ratio\n",
    "            predicted_word = output.argmax(1)\n",
    "            x = target[t] if teacher_force else predicted_word\n",
    "            \n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-throw",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(epochs, train_loss, val_loss):\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.plot(epochs, train_loss, label=\"Training Loss\")\n",
    "    plt.plot(epochs, val_loss, label=\"Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brilliant-edmonton",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(len(NAHUATL.vocab), \n",
    "                  hyperparameters[\"embedding_size\"], \n",
    "                  hyperparameters[\"hidden_size\"], \n",
    "                  hyperparameters[\"num_layers\"],\n",
    "                  hyperparameters[\"dropout\"]).to(device)\n",
    "\n",
    "decoder = Decoder(len(SPANISH.vocab), \n",
    "                  hyperparameters[\"embedding_size\"], \n",
    "                  hyperparameters[\"hidden_size\"], \n",
    "                  hyperparameters[\"num_layers\"], \n",
    "                  hyperparameters[\"dropout\"]).to(device)\n",
    "                            \n",
    "seq2seq = Seq2Seq(encoder, decoder).to(device)\n",
    "\n",
    "padding_index = SPANISH.vocab.stoi[SPANISH.pad_token]\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index = padding_index)\n",
    "optimizer = optim.Adam(\n",
    "    seq2seq.parameters(),\n",
    "    lr=hyperparameters[\"learning_rate\"]\n",
    ")\n",
    "\n",
    "epoch_list = []\n",
    "val_loss_list = []\n",
    "train_loss_list = []\n",
    "total_loss = 0\n",
    "\n",
    "for epoch in range(hyperparameters[\"epochs\"]):\n",
    "    \n",
    "    # TRAIN LOOP\n",
    "    training_loss = 0\n",
    "    seq2seq.train()\n",
    "    \n",
    "    for i, batch in enumerate(train_iter):\n",
    "        \n",
    "        source = batch.nahuatl.to(device)\n",
    "        target = batch.spanish.to(device)\n",
    "        \n",
    "        output = seq2seq(source, target)\n",
    "        output_reshaped = output[1:].reshape(-1, output.shape[2])\n",
    "        target_reshaped = target[1:].reshape(-1)\n",
    "        loss = loss_fn(output_reshaped, target_reshaped)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        training_loss += loss.item()\n",
    "    \n",
    "    # VALIDATION LOOP\n",
    "    validation_loss = 0\n",
    "    seq2seq.eval()\n",
    "    \n",
    "    for i, batch in enumerate(val_iter):\n",
    "        source = batch.nahuatl.to(device)\n",
    "        target = batch.spanish.to(device)\n",
    "        output = seq2seq(source, target)\n",
    "        output_reshaped = output[1:].reshape(-1, output.shape[2])\n",
    "        target_reshaped = target[1:].reshape(-1)\n",
    "        loss = loss_fn(output_reshaped, target_reshaped)\n",
    "        validation_loss += loss.item()\n",
    "    \n",
    "    epoch_list.append(epoch+1)\n",
    "    training_loss_avg = training_loss/len(train_iter)\n",
    "    train_loss_list.append(training_loss_avg)\n",
    "    validation_loss_avg = validation_loss/len(val_iter)\n",
    "    val_loss_list.append(validation_loss_avg)\n",
    "\n",
    "    print(\"Epoch: {}\".format(epoch+1))\n",
    "    print(\"Training loss: {}\".format(training_loss_avg))\n",
    "    print(\"Validation loss: {}\".format(validation_loss_avg))\n",
    "    \n",
    "plot_loss(epoch_list, train_loss_list, val_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-flight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(seq2seq.state_dict(), \"Seq2Seq\")\n",
    "# torch.save(encoder.state_dict(), \"Encoder\")\n",
    "# torch.save(decoder.state_dict(), \"Decoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-operator",
   "metadata": {},
   "source": [
    "### Part 3 - Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40566961",
   "metadata": {},
   "source": [
    "The correct hyperparameters for loading the model are:\n",
    "\n",
    "hyperparameters = {\"epochs\":30,\n",
    "                   \"batch_size\":64,\n",
    "                   \"embedding_size\":256,\n",
    "                   \"hidden_size\":1024,\n",
    "                   \"learning_rate\":0.001,\n",
    "                   \"num_layers\":2,\n",
    "                   \"dropout\":0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05989a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set load_model to True if you want to load the saved model\n",
    "load_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc634b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_model:\n",
    "    encoder = Encoder(len(NAHUATL.vocab), \n",
    "                  hyperparameters[\"embedding_size\"], \n",
    "                  hyperparameters[\"hidden_size\"], \n",
    "                  hyperparameters[\"num_layers\"],\n",
    "                  hyperparameters[\"dropout\"]).to(device)\n",
    "    decoder = Decoder(len(SPANISH.vocab), \n",
    "                  hyperparameters[\"embedding_size\"], \n",
    "                  hyperparameters[\"hidden_size\"], \n",
    "                  hyperparameters[\"num_layers\"], \n",
    "                  hyperparameters[\"dropout\"]).to(device)\n",
    "    \n",
    "    encoder.load_state_dict(torch.load(\"Encoder\"))\n",
    "    decoder.load_state_dict(torch.load(\"Decoder\"))\n",
    "    seq2seq = Seq2Seq(encoder, decoder).to(device).to(device)\n",
    "    seq2seq.load_state_dict(torch.load(\"Seq2Seq\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-disorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq.eval()\n",
    "total_loss = 0\n",
    "\n",
    "padding_index = SPANISH.vocab.stoi[SPANISH.pad_token]\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index = padding_index)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(test_iter):\n",
    "        \n",
    "        source = batch.nahuatl.to(device)\n",
    "        target = batch.spanish.to(device)\n",
    "        output = seq2seq(source, target)\n",
    "        output_reshaped = output[1:].reshape(-1, output.shape[2])\n",
    "        target_reshaped = target[1:].reshape(-1)\n",
    "\n",
    "        loss = loss_fn(output_reshaped, target_reshaped)\n",
    "        total_loss += loss.item()\n",
    "        print(f\"Loss is: {total_loss/(i+1)}\", end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1937fe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below are some sentences from the test data to test the model on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a838ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence_1 = \"miak otik yekitayah noso se tonali oixpolo\"\n",
    "# correct: la queríamos mucho pero un día desapareció"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0834a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence_2 = \"se kitoka iteyotsin\"\n",
    "# correct: se siembra la semillita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b905d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence_3 = \"wan itakilo tein chikawak semi welik mah se kimana wan mah se kitsopeli ika panela\"\n",
    "# correct: su fruto recio es muy sabroso hervido y endulzado con piloncillo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85479a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function translates a Nahuan sentence to Spanish\n",
    "\n",
    "def nahuatl_to_spanish(sentence):\n",
    "    n = elotl.nahuatl.orthography.Normalizer(\"inali\")\n",
    "    normalized_sentence = n.normalize(sentence)\n",
    "    tokenized_sentence = normalized_sentence.split(' ')\n",
    "    tokenized_sentence = [string.lower() for string in tokenized_sentence]\n",
    "    tokenized_sentence.insert(0, NAHUATL.init_token)\n",
    "    tokenized_sentence.append(NAHUATL.eos_token)\n",
    "    indiced_sentence = [NAHUATL.vocab.stoi[x] for x in tokenized_sentence]\n",
    "    \n",
    "    sentence_tensor = torch.LongTensor(indiced_sentence).unsqueeze(1).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        hidden, cell = seq2seq.encoder(sentence_tensor)\n",
    "\n",
    "    outputs = [NAHUATL.vocab.stoi[\"<start>\"]]\n",
    "\n",
    "    for _ in range(len(sentence)*3):\n",
    "        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, hidden, cell = seq2seq.decoder(previous_word, hidden, cell)\n",
    "            best_guess = output.argmax(1).item()\n",
    "\n",
    "        outputs.append(best_guess)\n",
    "\n",
    "        if output.argmax(1).item() == SPANISH.vocab.stoi[\"<end>\"]:\n",
    "            break\n",
    "\n",
    "    translated_sentence = [SPANISH.vocab.itos[idx] for idx in outputs]\n",
    "\n",
    "    return tokenized_sentence[1:-1], translated_sentence[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdfb390",
   "metadata": {},
   "outputs": [],
   "source": [
    "source, target = nahuatl_to_spanish(test_sentence_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58609385",
   "metadata": {},
   "outputs": [],
   "source": [
    "source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102e8250",
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf84a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function calculates BLEU score on the test data by comparing the generated translations \n",
    "# with the authentic ones from the corpus.\n",
    "\n",
    "def calculate_bleu_score(test_data):\n",
    "    \n",
    "    candidate_corpus = []\n",
    "    reference_corpus = []\n",
    "    sources = []\n",
    "    \n",
    "    for idx, row in test_data.iterrows():\n",
    "        source, target = nahuatl_to_spanish(row[\"Nah\"])\n",
    "        sources.append(source)\n",
    "        candidate_corpus.append(target)\n",
    "        reference_corpus.append([row[\"Esp\"].split()])\n",
    "    \n",
    "    bleu = bleu_score(candidate_corpus, reference_corpus)\n",
    "    \n",
    "    print(f\"Bleu score: {bleu}\")\n",
    "    \n",
    "    return candidate_corpus, reference_corpus, sources, bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ac38e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"test.csv\")\n",
    "candidate_corpus, reference_corpus, sources, bleu = calculate_bleu_score(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779da0b4",
   "metadata": {},
   "source": [
    "The code below is used to investigate the length of the sentences in the corpus and calculate the BLEU score for sentences of different length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d07df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv(\"Axolotl.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b990ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.asarray([len(sent.split()) for sent in corpus[\"Esp\"]])\n",
    "print(x[x.argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856f3bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "extremely_long_sentences = np.asarray([1 for sent in corpus[\"Esp\"] if len(sent.split()) > 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5a0a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "extremely_long_sentences.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6b629c",
   "metadata": {},
   "outputs": [],
   "source": [
    "very_long_sentences = np.asarray([1 for sent in corpus[\"Esp\"] if len(sent.split()) > 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cc216e",
   "metadata": {},
   "outputs": [],
   "source": [
    "very_long_sentences.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600b50f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_sentences = np.asarray([1 for sent in corpus[\"Esp\"] if len(sent.split()) > 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57f5a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_sentences.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67355e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_word_sentences = np.asarray([1 for sent in corpus[\"Esp\"] if len(sent.split()) == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d12495",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_word_sentences.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c57a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = test_data.Esp.str.len().sort_values().index\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1b0b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.reindex(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb2e318",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = test_data.Esp.str.len().sort_values().index\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707b309d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "for idx, row in test_data.reindex(s).iterrows():\n",
    "    if len(row[\"Esp\"].split()) > 75:\n",
    "        n += 1\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb89cfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_bleu_score(test_data.reindex(s)[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752ef6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_bleu_score(test_data.reindex(s)[100:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2fafe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_bleu_score(test_data.reindex(s)[200:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b4b3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_bleu_score(test_data.reindex(s)[300:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b10d116",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_bleu_score(test_data.reindex(s)[400:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd010fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_bleu_score(test_data.reindex(s)[500:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-african",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "Grupo de Ingeniería Lingüística GIL, UNAM. (2015). Corpus paralelo español-náhuatl. [Dataset]. http://www.corpus.unam.mx/axolotl\n",
    "\n",
    "Trevett, B. (2018). Sequence to Sequence Learning with Neural Networks. [Jupyter Notebook]. https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
